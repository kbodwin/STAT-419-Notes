[
["index.html", "STAT 419 Lecture Notes Course Policies", " STAT 419 Lecture Notes Kelly Bodwin 2019-04-10 Course Policies "],
["intro.html", "Chapter 1 Introduction Example: Paired t-tests Goals of this class Just for fun: My research", " Chapter 1 Introduction The heart and soul of multivariate statistics is relationships between variables. How do we describe and display them? How do we measure and estimate them? How do we test them? How do we account for them? Example: Paired t-tests Recall from your introductory Statistics class: two-sample t-tests compare the means of two independent samples a paired t-test compares the mean difference from two dependent samples Consider Midterm grades in Section 01 versus midterm grades in Section 02 Midterm grades in Stat 419 versus final grades in Stat 419 names midterm_grades final_grades Mario 85 89 Luigi 90 93 Wario 65 61 Waluigi 72 78 Bowser 41 47 with(grades, t.test(midterm_grades, final_grades, alternative = &quot;less&quot;, paired = FALSE) ) ## ## Welch Two Sample t-test ## ## data: midterm_grades and final_grades ## t = -0.24526, df = 8, p-value = 0.4062 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 19.74586 ## sample estimates: ## mean of x mean of y ## 70.6 73.6 with(grades, t.test(midterm_grades, final_grades, alternative = &quot;less&quot;, paired = TRUE) ) ## ## Paired t-test ## ## data: midterm_grades and final_grades ## t = -1.627, df = 4, p-value = 0.08954 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.9309312 ## sample estimates: ## mean of the differences ## -3 This time, we accounted for the relationship between an individual’s two exam scores! grades %&gt;% summarize_at( vars(midterm_grades, final_grades), funs(sd, mean)) ## midterm_grades_sd final_grades_sd midterm_grades_mean final_grades_mean ## 1 19.32097 19.35975 70.6 73.6 The variance of the difference of means is: \\[\\text{var}(\\bar{X}_2 - \\bar{X}_1) = (\\sigma_1^2 + \\sigma_2^2)/n\\] var &lt;- (19.3^2 + 19.4^2)/5 var ## [1] 149.77 sqrt(var) ## [1] 12.23806 But… they are not independent! grades %&gt;% select(midterm_grades, final_grades) %&gt;% cov() ## midterm_grades final_grades ## midterm_grades 373.30 365.55 ## final_grades 365.55 374.80 The equation is actually: \\[\\text{var}(\\bar{X}_2 - \\bar{X}_1) = (\\sigma_1^2 + \\sigma_2^2 - 2 \\, \\sigma_{12})/n\\] actual_var &lt;- (19.3^2 + 19.4^2 - 2*365)/5 actual_var ## [1] 3.77 sqrt(actual_var) ## [1] 1.941649 Goals of this class Expand your basic stats knowledge into the magical world of multiple variables. Teach you to responsibly consider and test variable relationships. Show you classic and cutting edge methods for multivariate data. Have some fun with interesting datasets! Just for fun: My research Data from last.fm music streaming site. Samples: Listeners \\((n = 1893)\\) Variables: Artists \\((d = 17,632)\\) Goal: Given an artist, recommend related artists based on listener data. Many variables (whether or not a listener listened to each artist), very dependent! Result 1: Paul McCartney Artist Top 5 Tags Paul McCartney sad, classic rock, cool, british, beautiful The Beatles 60s, classic rock, british, psychedelic, &lt;3 George Harrison classic rock, 70s, singer-songwriter, sad, british John Lennon classic rock, singer-songwriter, 70s, british, male vocalists Result 2: Hannah Montana Artist Top 5 Tags Hannah Montana love at first listen, pop rock, soundtrack, amazing, female vocalist Miley Cyrus &lt;3, catchy, love at first listen, amazing, pop rock Rihanna rnb, ballad, sexy, love, dance Katy Perry pop rock, &lt;3, catchy, love, love at first listen Britney Spears catchy, female, sexy, amazing, dance Ke$ha love at first listen, dance, &lt;3, pop, catchy Lady Gaga dance, female vocalist, love at first listen, catchy, sexy Demi Lovato love at first listen, &lt;3, pop rock, catchy, female vocalist Avril Lavigne pop rock, canadian, pop punk, female, love at first listen Taylor Swift country, &lt;3, catchy, love, amazing Selena Gomez &lt;3, pop rock, love at first listen, catchy, love Ashley Tisdale &lt;3, catchy, pop rock, ballad, awesome Hilary Duff favorites, amazing, sexy, pop rock, dance Christina Aguilera ballad, sexy, soul, rnb, amazing Jonas Brothers pop rock, &lt;3, love, love at first listen, amazing Beyonce rnb, sexy, soul, ballad, female vocalist Glee Cast cover, love at first listen, love, catchy, soundtrack "],
["matrix-algebra.html", "Chapter 2 Matrix Algebra Scalars Vectors Matrices Matrix multiplication Trace, Determinant, Inverse Eigenvalues and Singular Values Summary Statistics", " Chapter 2 Matrix Algebra .example { background-color: cornsilk; color: blue; } .q { background-color: cornsilk; color: darkred; text-align: right; } .alert { background-color: light red; color: dark blue; font: bold; text-align: center; } button{ margin:auto; display:block; float: right; } Scalars A scalar is a single number by itself. In statistics, a scalar will usually be: A single data observation A parameter A summary statistic Notation and Examples Try to come up with your own real world examples that each of the following scalars might be representing. Then click to see my example. A single observation: \\[ x = 90 \\] Example Yao Ming is 7 feet 6 inches tall. A parameter: \\[ \\mu = 67 \\] Example The average height of all people in the United States is 5 feet 7 inches. Summary statistics: \\[ \\bar{x} = 79, \\; s_x = 7.96\\] Example The average height of ten random NBA players is 6 feet 7 inches, with a standard deviation of 7.96. Multiple observations: \\[ a_1 = 8, a_2 = 3\\] \\[ b_1 = 4, b_2 = 1 \\] Example Ex 3: My roommate Marie and I went to see Wonder Woman. She rated it as 8 out of 10, and I rated it as 4 out of 10. Later, we watched the movie Suicide Squad. She gave it a 3 out of 10, and I gave it a 1 out of 10. Calculations with scalars x &lt;- 90 mu &lt;- 72 nba_sample &lt;- c(86, 70, 76, 88, 78, 84, 70, 90, 68, 80) x_bar &lt;- mean(nba_sample) sx &lt;- sd(nba_sample) a_1 &lt;- 8 a_2 &lt;- 3 b_1 &lt;- 4 b_2 &lt;- 1 This is pretty straightforward, by hand or in R. avg_ww &lt;- (a_1 + b_1)/2 avg_ww ## [1] 6 The average rating given to Wonder Woman was \\((a_1 + b_1)/2\\) = 6 t &lt;- (x_bar - mu)/(sx/sqrt(10)) t ## [1] 2.781518 There is fairly strong evidence that NBA players tend to be taller than average (t-score of \\(t = \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{10}}\\) = 2.78). Vectors A vector is a set of scalars put together. In statistics, a vector might be a set of samples from a single variable or a set of observations of many variables from a single sample. Notation and Examples Marie’s ratings and Kelly’s ratings of two movies, Wonder Woman and Suicide Squad \\[{\\bf a} = \\left( \\matrix{a_1 \\\\ a_2} \\right) = \\left( \\matrix{8 \\\\ 3} \\right)\\] \\[{\\bf b} = \\left( \\matrix{b_1 \\\\ b_2} \\right) = \\left( \\matrix{4 \\\\ 1} \\right)\\] Sample of 10 NBA heights in inches: 86, 70, 76, 88, 78, 84, 70, 90, 68, 80 \\[{\\bf h} = (86, 70, 76, 88, 78, 84, 70, 90, 68, 80)^t\\] \\[{\\bf h} = (86, 70, 76, 88, 78, 84, 70, 90, 68, 80)^{&#39;}\\] ??? What is the deal with the “t” or the apostrophe? Calculations with vectors nba_sample &lt;- c(86, 70, 76, 88, 78, 84, 70, 90, 68, 80) mean(nba_sample) ## [1] 79 sd(nba_sample) ## [1] 7.958224 In our sample of 10 NBA players, the mean height was 79 and the sample standard deviation was 7.9582243. a &lt;- c(a_1, a_2) b &lt;- c(b_1, b_2) mean(a) ## [1] 5.5 mean(b) ## [1] 2.5 Marie rated DC movies at 5.5 on average. Kelly rated the DC movies at 2.5 on average. Matrices A matrix is a two dimensional set of scalars; or equivalently, many vectors put together. Some vocabulary about matrices: The dimension of a matrix (\\(m \\times n\\)) is the number of rows (\\(m\\)) by the number of columns (\\(n\\)). The elements of the matrix are often written as \\(a_{ij}\\), as in \\[ {\\bf A} \\, = \\, \\left( \\matrix{ a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} } \\right) \\] A \\(1 \\times n\\) matrix is called a row vector. A \\(m \\times 1\\) matrix is called a column vector. A square matrix has the same number of rows as columns. A diagonal matrix has all zeros except on the diagonal. Special Definitions: \\({\\bf 1_n}\\) is a column vector of \\(n\\) ones: \\[{\\bf 1_n} = \\left( \\matrix{1 \\\\ 1 \\\\ \\vdots \\\\ 1} \\right)\\] one = rep(1, 3) one ## [1] 1 1 1 \\({\\bf I_n}\\) is called the \\(n \\times n\\) identity matrix: \\[{\\bf I_n} = \\left( \\matrix{1 &amp; 0 &amp; \\ldots \\\\ 0 &amp; 1 &amp; \\ldots \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\ldots &amp; 0 &amp; 1} \\right)\\] I = diag(3) I ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 \\({\\bf J_n}\\) is called the \\(n \\times n\\) ones matrix: \\[{\\bf J_n} = \\left( \\matrix{1 &amp; 1 &amp; \\ldots \\\\ 1 &amp; 1 &amp; \\ldots \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\ldots &amp; 1 &amp; 1} \\right)\\] J = matrix(1, 3, 3) J ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 ## [3,] 1 1 1 Examples Click to see the matrix of movie ratings. My roommate Marie and I went to see Wonder Woman. She rated it as 8 out of 10, and I rated it as 4 out of 10. Later, we watched the movie Suicide Squad. She gave it a 3 out of 10, and I gave it a 1 out of 10. \\[M = \\left(\\matrix{{\\bf a} \\\\ \\bf{b}} \\right) = \\left( \\matrix{a_1 &amp; a_2 \\\\ b_1 &amp; b_2} \\right) = \\left( \\matrix{8 &amp; 3 \\\\ 4 &amp; 1} \\right)\\] Matrix multiplication [Whiteboard] M = rbind(a,b) M ## [,1] [,2] ## a 8 3 ## b 4 1 H = matrix(c(1,2,3,4), c(2,2)) H ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 M*H ## [,1] [,2] ## a 8 9 ## b 8 4 M %*% H ## [,1] [,2] ## a 14 36 ## b 6 16 H %*% M ## [,1] [,2] ## [1,] 20 6 ## [2,] 32 10 !! WARNING !! Not all matrices can be multiplied - the inner dimensions must line up! Example Suppose we have the following matrices with the given dimensions: A is a 2 x 20 matrix, C is 320 x 5, F is 156 x 1, B is 20 x 320, E is 14 x 156, G is 1 x 25, and H is 25 x 2, D is 5 x 14. Find the dimension of the matrix resulting from the following product: ABCDEFGH. Trace, Determinant, Inverse The trace of a matrix is the sum of its diagonal elements: sum(diag(M)) ## [1] 9 The determinant of a matrix is a measure of (in some sense) its ``size“. It is found by subtracting the off-diagonal elements from the diagonal elements. [Whiteboard] det(M) ## [1] -4 If the determinant of a matrix is not a negative number, we call it non-negative definite. If the determinant of a matrix is not a negative number or zero, we call it positive definite. The inverse of a matrix is what we can multiply it by to get the identity matrix: \\[ {\\bf A}^{-1} {\\bf A} = {\\bf I}\\] [Whiteboard] solve(M) ## a b ## [1,] -0.25 0.75 ## [2,] 1.00 -2.00 !! WARNING !! Only square matrices have an inverse! (why?) Eigenvalues and Singular Values The eigenvalues of a square matrix are a set of up to \\(n\\) scalars that characterize the matrix in a few mathematically important ways. We will see more interpretations of these later. For now, we will practice calculating them. [Whiteboard] eigen(M) ## eigen() decomposition ## $values ## [1] 9.4244289 -0.4244289 ## ## $vectors ## [,1] [,2] ## [1,] 0.9033441 -0.3354710 ## [2,] 0.4289165 0.9420505 The number of nonzero eigenvalues of a matrix is called the rank of the matrix. The sum of the eigenvalues is the equal to the trace of the matrix. The product of the eigenvalues is equal to the determinant of the matrix. sum(eigen(M)$values) ## [1] 9 sum(diag(M)) ## [1] 9 prod(eigen(M)$values) ## [1] -4 det(M) ## [1] -4 Quick Quiz Is M positive definite? Non-negative definite? The singular values of a matrix are a set of up to \\(m\\) or \\(n\\) scalars that characterize the matrix in a few mathematically important ways. We find the singular values by taking the square root of the eigenvalues of the “squared” matrix: \\[ \\text{sing. vals. of } {\\bf A} = \\sqrt{ \\text{eigen}( {\\bf A}&#39; {\\bf A}) }\\] [Whiteboard] Important fact Every matrix \\(A\\) can be decomposed into a diagonal matrix \\(D\\) of its singular values, plus two extra matrices: \\[ {\\bf A} = {\\bf U} {\\bf D} {\\bf V}&#39;\\] (You will not be asked to find this decomposition by hand.) svd(M) ## $d ## [1] 9.4774400 0.4220549 ## ## $u ## [,1] [,2] ## [1,] -0.9013032 -0.4331887 ## [2,] -0.4331887 0.9013032 ## ## $v ## [,1] [,2] ## [1,] -0.9436283 0.3310069 ## [2,] -0.3310069 -0.9436283 Summary Statistics So, how do all these matrix skills fit in to the world of statistics? We typically think of data as being a collection of samples from variables. Suppose we have n samples and m variables; then, we have an \\(n \\times m\\) matrix of data. ??? Wait… n by m or m by n??? For multivariate data, we can use matrix algebra on our data matrix to find important summaries and descpritions. Means of Vectors Recall from univariate statistics that the sample mean of some samples \\(x_1, ..., x_n\\) is \\[ \\bar{x} = \\frac{x_1 + x_2 + ... + x_n}{n} \\] If we want, we can represent our data as a vector, and then find the mean via matrix algebra! \\[ {\\bf x} = (x_1, x_2, ..., x_n)&#39; \\] \\[ \\bar{x} = {\\bf x}&#39;{\\bf 1}*(1/n) \\] eric &lt;- c(5, 9) (t(eric) %*% c(1,1))/2 ## [,1] ## [1,] 7 Vectors of Means More importantly, we might be interested in the means of many different variables, across samples. The mean vector is the \\(m \\times 1\\) vector of column means. \\[ {\\bf \\bar{y}} = \\left( \\matrix{ \\bar{y}_{\\cdot 1} \\\\ \\vdots \\\\ \\bar{y}_{\\cdot m}}\\right) \\] [whiteboard] M &lt;- rbind(M, eric) colMeans(M) ## [1] 5.666667 4.333333 Covariance and correlation matrices Recall from univariate statistics that the variance of a variable is its spread and the covariance of two random variables is their relationship. \\[s_x = \\frac{1}{n-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\] \\[s_{xy} = \\frac{1}{n-1} \\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\] a ## [1] 8 3 b ## [1] 4 1 eric ## [1] 5 9 a - mean(a) ## [1] 2.5 -2.5 b - mean(b) ## [1] 1.5 -1.5 eric - mean(eric) ## [1] -2 2 cov(a, b) ## [1] 7.5 cov(b, eric) ## [1] -6 [Whiteboard] The covariance matrix is the \\(m \\times m\\) matrix containing all possible variances and covariances of a set of variables. \\[ \\Sigma = \\left( \\matrix{ \\sigma^2_{1} &amp; \\sigma_{12} &amp; \\ldots &amp; \\sigma_{1m} \\\\ &amp; \\vdots &amp; \\vdots &amp; \\\\ \\sigma_{m1} &amp; \\sigma_{m2} &amp; \\ldots &amp; \\sigma^2_{m} } \\right) \\] [whiteboard] cov(M) ## [,1] [,2] ## [1,] 4.3333333 -0.3333333 ## [2,] -0.3333333 17.3333333 Fun Fact Covariance matrices are always non-negative definite. (why?) Quick Quiz What does it mean if the determinant of a covariance matrix is 0? Recall from univariate statistics that the correlation is the covariance of two variables divided by the two standard deviations cov(a, b) ## [1] 7.5 cov(b, eric) ## [1] -6 sd(a) ## [1] 3.535534 sd(b) ## [1] 2.12132 sd(eric) ## [1] 2.828427 cor(a, b) ## [1] 1 cor(b, eric) ## [1] -1 The correlation matrix is the \\(m \\times m\\) matrix containing all correlations. [whiteboard] cor(M) ## [,1] [,2] ## [1,] 1.00000000 -0.03846154 ## [2,] -0.03846154 1.00000000 Challenge Write the correlation matrix as a matrix multiplication. Measures of Dispersion We have two overall summaries for the “dispersion”, or spread, of a set of variables: The total variance is the sum of the variances of each variable. In matrix algebra speak, this is the __ trace of the matrix. The generalized variance is the joint spread, i.e., the variances balanced by the covariances. In matrix algebra, this is the __ determinant of the matrix. "],
["characterizing-and-summarizing-multivariate-data.html", "Chapter 3 Characterizing and Summarizing Multivariate Data Linear Combinations of Random Variables Practice", " Chapter 3 Characterizing and Summarizing Multivariate Data .example { background-color: cornsilk; color: blue; } .q { background-color: cornsilk; color: darkred; text-align: right; } .alert { background-color: light red; color: dark blue; font: bold; text-align: center; } button{ margin:auto; display:block; float: right; } library(knitr) source(&quot;/Users/kbodwin/Dropbox/Teaching/kb_sweave_extras.R&quot;) Linear Combinations of Random Variables Suppose we have some constants (scalars) \\(a_1,a_2,\\ldots,a_p\\), and some variables \\(y_1,y_2,\\ldots,y_p\\). A linear combination of the \\(p\\) elements of \\({\\bf y}\\), denoted \\(z\\) is given by: \\[z=a_1y_1+a_2y_2+\\cdots+a_py_p \\, = \\, {\\bf a}&#39;{\\bf y}\\] !!! z is a scalar (or univariate random variable) For example, suppose we have a sample of 5 students’ scores on 8 assignments - 5 homeworks, 2 midterms, and a final. The \\(5 \\times 8\\) data matrix is given by: HW1 HW2 HW3 HW4 HW5 M1 M2 FIN 80 90 85 70 100 65 65 75 75 95 80 80 90 75 60 70 80 80 75 70 100 90 75 85 70 60 100 80 95 80 90 90 55 80 75 65 90 70 80 75 \\[{\\bf Y} = \\left( \\begin{array} 80 &amp; 90 &amp; 85 &amp; 70 &amp; 100 &amp; 65 &amp; 65 &amp; 75 \\\\\\\\ 75 &amp; 95 &amp; 80 &amp; 80 &amp; 90 &amp; 75 &amp; 60 &amp; 70 \\\\\\\\ 80 &amp; 80 &amp; 75 &amp; 70 &amp; 100 &amp; 90 &amp; 75 &amp; 85 \\\\\\\\ 70 &amp; 60 &amp; 100 &amp; 80 &amp; 95 &amp; 80 &amp; 90 &amp; 90 \\\\\\\\ 55 &amp; 80 &amp; 75 &amp; 65 &amp; 90 &amp; 70 &amp; 80 &amp; 75\\end{array} \\right)\\] Suppose we consider the grading scheme that gives equal weight to each assignment: \\[z=\\frac{1}{8}(y_1+y_2+y_3+y_4+y_5+y_6+y_7+y_8)\\] Then, Student 1 has a final grade of grade_1 &lt;- sum(scores[1,])/8 grade_1 ## [1] 78.75 \\[z=\\frac{1}{8}(80 + 90 + 85 + 70 + 100 + 65 + 65 + 75) = 78.75\\] If we wanted to calculate this grade average for all students simultaneously, we could represent this as a matrix calculation: \\[\\overline{{\\bf z}} = {\\bf j}&#39;{\\bf Y}\\] rowMeans(scores) ## [1] 78.750 78.125 81.875 83.125 73.750 Variances of linear combinations Recall that the covariance of a data matrix \\({\\bf Y}\\) shows you the variances and covariances between all the variables: cov(scores) ## HW1 HW2 HW3 HW4 HW5 M1 M2 FIN ## HW1 107.50 41.25 11.25 23.75 37.50 28.75 -60.00 8.75 ## HW2 41.25 180.00 -91.25 -16.25 -6.25 -51.25 -155.00 -98.75 ## HW3 11.25 -91.25 107.50 45.00 6.25 -3.75 60.00 47.50 ## HW4 23.75 -16.25 45.00 45.00 -6.25 15.00 -2.50 10.00 ## HW5 37.50 -6.25 6.25 -6.25 25.00 12.50 0.00 18.75 ## M1 28.75 -51.25 -3.75 15.00 12.50 92.50 38.75 51.25 ## M2 -60.00 -155.00 60.00 -2.50 0.00 38.75 142.50 80.00 ## FIN 8.75 -98.75 47.50 10.00 18.75 51.25 80.00 67.50 Do students who do better on M1 and M2 tend to do better on the Final? Yes! The covariances are positive - 51.25 and 80. Now, we’d like to know what the variance is average grades is; i.e., the variance of the univariate variable \\(z\\). What is wrong with the approach below? We didn’t account for the relationships between the variables!!! zs &lt;- rowMeans(scores) var(zs) ## [1] 13.39844 Instead, we need to use our matrix representation to think about the variance. In univariate statistics, when you multiply a variable by a constant, you have to square the constant and multiply by the variance: \\[\\text{var}(aX) = a^2 \\text{var}(X)\\] The same idea applies in mulivariate statistics: when you matrix multiply a data matrix by a constant vector, you have to “square” the vector and multiply by the covariance. \\[ \\text{var}(z) = \\text{var}({\\bf Y}{\\bf j}) \\, = \\, {\\bf j}&#39; \\text{Cov}(Y) {\\bf j}\\] \\[ s^2(z) \\, = \\, {\\bf j}&#39; S_Y {\\bf j}\\] ??? This is just a math rule - which you might derive in other classes! What is the dimension of var(z)? 1x1, it is a scalar! Practice Now consider the two weighting schemes for final grades: First, the “average all scores” approach we saw above. Second, a scheme where homework is worth 20%, midterms are worth 30%, and the final is worth 50%. \\[z^{(1)}=\\frac{1}{8}(y_1+y_2+y_3+y_4+y_5+y_6+y_7+y_8)\\] \\[z^{(2)}=\\frac{1}{5}(y_1+y_2+y_3+y_4+y_5)\\cdot0.2+\\frac{1}{2}(y_6+y_7)\\cdot0.3+y_8\\cdot 0.5\\] We can write this in matrix form as \\[ z_2 = {\\bf a}&#39;{\\bf y}\\] where \\({\\bf a}\\) is… Click for answer a = (0.04, 0.04, 0.04, 0.04, 0.04, 0.15, 0.15, 0.5) Thus, we can still use our matrix tricks - with \\({\\bf a}\\) instead of \\({\\bf j}\\) - to calculate everyone’s scores, as well as the covariances! \\[{\\bf z} = {\\bf Y}{\\bf a}\\] \\[s^2_z = {\\bf a}&#39; S_Y {\\bf a}\\] a &lt;- c(0.04, 0.04, 0.04, 0.04, 0.04, 0.15, 0.15, 0.5) Y &lt;- as.matrix(scores) all_grades &lt;- Y %*% a all_grades ## [,1] ## [1,] 74.00 ## [2,] 72.05 ## [3,] 83.45 ## [4,] 86.70 ## [5,] 74.60 var_grades &lt;- t(a) %*% var(Y) %*% a var_grades ## [,1] ## [1,] 42.05675 sqrt(var_grades) ## [,1] ## [1,] 6.485118 "]
]
